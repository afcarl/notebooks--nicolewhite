{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Learning Framework Comparison\n",
    "\n",
    "**Logistic Regression** implemented in:\n",
    "\n",
    "* Theano\n",
    "* Tensorflow\n",
    "* PyTorch\n",
    "* Neon\n",
    "* Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "num_samples = 100\n",
    "num_feats = 40\n",
    "epochs = 10000\n",
    "learning_rate = .01\n",
    "\n",
    "x_train = np.random.normal(size=(num_samples, num_feats))\n",
    "\n",
    "y_train = np.random.randint(2, size=(num_samples))\n",
    "y_train_2dim = y_train.reshape(num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 40)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_2dim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Static Computation Graphs\n",
    "\n",
    "Theano and Tensorflow require *static* computation graphs; they are defined *once* and executed over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Theano\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Shared variables](http://deeplearning.net/software/theano_versions/dev/tutorial/examples.html#using-shared-variables)\n",
    "* [Typed constructors](http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors)\n",
    "* [Gradients](http://deeplearning.net/software/theano/tutorial/gradients.html)\n",
    "* [Functions](http://deeplearning.net/software/theano/library/compile/function.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.dmatrix('x')\n",
    "y = T.dvector('y')\n",
    "\n",
    "w = theano.shared(np.zeros(num_feats))\n",
    "b = theano.shared(0.)\n",
    "\n",
    "yhat = T.nnet.sigmoid(T.dot(x, w) + b)\n",
    "loss = T.nnet.binary_crossentropy(output=yhat, target=y)\n",
    "cost = loss.mean()\n",
    "\n",
    "dw, db = T.grad(cost, [w, b])\n",
    "\n",
    "train = theano.function(\n",
    "  inputs=[x, y],\n",
    "  outputs=cost,\n",
    "  updates=((w, w - learning_rate * dw), (b, b - learning_rate * db))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.6931471805599458\n",
      "Mean Loss 0.5136904630014473\n",
      "Mean Loss 0.49878517969608693\n",
      "Mean Loss 0.49352835203491163\n",
      "Mean Loss 0.491066829816844\n",
      "Mean Loss 0.48975355712923296\n",
      "Mean Loss 0.4889947104353848\n",
      "Mean Loss 0.48853172240666787\n",
      "Mean Loss 0.4882380835373335\n",
      "Mean Loss 0.4880465157264737\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    mean_loss = train(x=x_train, y=y_train)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('Mean Loss', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorflow\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Variables](https://www.tensorflow.org/programmers_guide/variables)\n",
    "* [Placeholders](https://www.tensorflow.org/api_guides/python/reading_data#feeding)\n",
    "* [Gradients](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_computation)\n",
    "* [Session](https://www.tensorflow.org/api_docs/python/tf/Session#run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(num_samples, num_feats))\n",
    "y = tf.placeholder(tf.float32, shape=(num_samples, 1))\n",
    "\n",
    "w = tf.Variable(tf.zeros((num_feats, 1)))\n",
    "b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "yhat = tf.sigmoid(tf.matmul(x, w) + b)\n",
    "loss = tf.losses.log_loss(labels=y, predictions=yhat)\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "dw, db = tf.gradients(cost, [w, b])\n",
    "\n",
    "new_w = w.assign(w - learning_rate * dw)\n",
    "new_b = b.assign(b - learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.693147\n",
      "Mean Loss 0.51369\n",
      "Mean Loss 0.498785\n",
      "Mean Loss 0.493528\n",
      "Mean Loss 0.491067\n",
      "Mean Loss 0.489753\n",
      "Mean Loss 0.488995\n",
      "Mean Loss 0.488532\n",
      "Mean Loss 0.488238\n",
      "Mean Loss 0.488046\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        mean_loss, _, _ = sess.run([cost, new_w, new_b], feed_dict={x: x_train, y: y_train_2dim})\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print('Mean Loss', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dynamic Computation Graphs\n",
    "\n",
    "PyTorch and Neon offer \"true\" automatic differientiation. The computation graph is *dynamic*; it is constructed on the fly during each epoch as we calculate the loss and weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PyTorch\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Variables and autograd](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd)\n",
    "* [Static vs dynamic graphs](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#tensorflow-static-graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "x = Variable(torch.from_numpy(x_train).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.from_numpy(y_train_2dim).type(dtype), requires_grad=False)\n",
    "\n",
    "w = Variable(torch.zeros(num_feats, 1), requires_grad=True)\n",
    "b = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.6931465268135071\n",
      "Mean Loss 0.5136904716491699\n",
      "Mean Loss 0.49878519773483276\n",
      "Mean Loss 0.4935283362865448\n",
      "Mean Loss 0.49106690287590027\n",
      "Mean Loss 0.48975348472595215\n",
      "Mean Loss 0.4889947175979614\n",
      "Mean Loss 0.48853182792663574\n",
      "Mean Loss 0.48823806643486023\n",
      "Mean Loss 0.48804646730422974\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    yhat = torch.sigmoid(x.mm(w) + b)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    cost = torch.mean(loss)\n",
    "    \n",
    "    cost.backward()  # <-- This is neat.\n",
    "\n",
    "    w.data -= learning_rate * w.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('Mean Loss', cost.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neon\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Neon backend](https://neon.nervanasys.com/index.html/backends.html)\n",
    "* [Automatic differentiation](https://neon.nervanasys.com/index.html/backends.html#automatic-differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DISPLAY:neon:mklEngine.so not found; falling back to cpu backend\n"
     ]
    }
   ],
   "source": [
    "from neon.backends import gen_backend, Autodiff\n",
    "\n",
    "be = gen_backend('cpu')\n",
    "\n",
    "w = be.zeros(num_feats)\n",
    "b = be.zeros(1)\n",
    "\n",
    "x = be.empty_like(x_train)\n",
    "y = be.empty_like(y_train)\n",
    "x[:] = x_train\n",
    "y[:] = y_train\n",
    "\n",
    "def loss_fn(yhat, y):\n",
    "    # Binary cross entropy.\n",
    "    return -y * be.log(yhat) - (1 - y) * be.log(1 - yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.692147\n",
      "Mean Loss 0.513663\n",
      "Mean Loss 0.498777\n",
      "Mean Loss 0.493525\n",
      "Mean Loss 0.491065\n",
      "Mean Loss 0.489753\n",
      "Mean Loss 0.488994\n",
      "Mean Loss 0.488531\n",
      "Mean Loss 0.488238\n",
      "Mean Loss 0.488046\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    yhat = be.sig(be.dot(x, w) + b)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    cost = be.mean(loss)\n",
    "\n",
    "    ad = Autodiff(op_tree=cost, be=be)\n",
    "    dw, db = ad.get_grad_tensor([w, b])\n",
    "    \n",
    "    w[:] = w - learning_rate * dw\n",
    "    b[:] = b - learning_rate * db\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('Mean Loss', cost.asnumpyarray()[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Abstractions\n",
    "\n",
    "### Keras\n",
    "\n",
    "External abstractions for Theano and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(num_feats,)))\n",
    "\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80225438],\n",
       "       [ 0.40147239],\n",
       "       [ 0.85430479],\n",
       "       [ 0.59398532],\n",
       "       [ 0.74547029]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=epochs, batch_size=num_samples, verbose=0)\n",
    "model.predict(x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorflow's [Estimator](https://www.tensorflow.org/extend/estimators) API\n",
    "\n",
    "Internal abstractions for Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "Estimator = tf.estimator.Estimator\n",
    "EstimatorSpec = tf.estimator.EstimatorSpec\n",
    "ModeKeys = tf.estimator.ModeKeys\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    x = features['x']\n",
    "    yhat = tf.layers.dense(inputs=x, units=1, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    if mode == ModeKeys.PREDICT:\n",
    "        return EstimatorSpec(mode=mode, predictions={'probs': yhat})\n",
    "    \n",
    "    loss = tf.losses.log_loss(labels=labels, predictions=yhat)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    if mode == ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss=cost, global_step=tf.train.get_global_step())\n",
    "        return EstimatorSpec(mode=mode, loss=cost, train_op=train_op)\n",
    "    else:\n",
    "        # Eval mode.\n",
    "        return EstimatorSpec(mode=mode, loss=cost)\n",
    "    \n",
    "\n",
    "model = Estimator(model_fn=model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x117b51518>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_input_fn = tf.estimator.inputs.numpy_input_fn\n",
    "\n",
    "train_input_fn = numpy_input_fn(x={'x': x_train}, y=y_train_2dim, num_epochs=None, shuffle=True)\n",
    "model.train(input_fn=train_input_fn, steps=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'probs': array([ 0.80095333])},\n",
       " {'probs': array([ 0.39785189])},\n",
       " {'probs': array([ 0.85093534])},\n",
       " {'probs': array([ 0.59857714])},\n",
       " {'probs': array([ 0.74323036])}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_input_fn = numpy_input_fn(x={'x': x_train[:5]}, num_epochs=1, shuffle=False)\n",
    "list(model.predict(input_fn=predict_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PyTorch's [nn](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module) module\n",
    "\n",
    "Internal abstractions for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, Sigmoid, BCELoss\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "x = Variable(torch.from_numpy(x_train).type(dtype))\n",
    "y = Variable(torch.from_numpy(y_train_2dim).type(dtype))\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(in_features=num_feats, out_features=1),\n",
    "    Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.)\n",
    "loss_fn = BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    yhat = model(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    cost = torch.mean(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    x = Variable(torch.from_numpy(x).type(torch.FloatTensor))\n",
    "    yhat = model(x)\n",
    "    return yhat.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80273807],\n",
       "       [ 0.39796931],\n",
       "       [ 0.85340548],\n",
       "       [ 0.59930348],\n",
       "       [ 0.74230564]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neon\n",
    "\n",
    "Internal abstractions for Neon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from neon.layers import Affine, GeneralizedCost\n",
    "from neon.initializers import Constant\n",
    "from neon.transforms import Logistic, CrossEntropyBinary\n",
    "from neon.transforms.cost import Cost\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum\n",
    "from neon.data import ArrayIterator\n",
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend('cpu', batch_size=num_samples)\n",
    "\n",
    "model = Model([\n",
    "    Affine(nout=1, init=Constant(0.), bias=Constant(0.), activation=Logistic())\n",
    "])\n",
    "\n",
    "cost = GeneralizedCost(CrossEntropyBinary())\n",
    "optimizer = GradientDescentMomentum(learning_rate=learning_rate, momentum_coef=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = ArrayIterator(x_train, y_train_2dim, make_onehot=False)\n",
    "model.fit(dataset, cost=cost, optimizer=optimizer, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # The tensor passed through fprop needs to have the \n",
    "    # same size as the batch size used during training.\n",
    "    # There must be a better way...\n",
    "    original_x_len = x.shape[0]\n",
    "    \n",
    "    padded_x = np.concatenate((x, np.zeros((be.bsz - original_x_len, num_feats))), axis=0)\n",
    "    padded_x_tensor = be.empty_like(padded_x.T)\n",
    "    padded_x_tensor[:] = padded_x.T\n",
    "    \n",
    "    output = model.fprop(padded_x_tensor)\n",
    "    output = output.asnumpyarray()[0]\n",
    "    \n",
    "    return output[:original_x_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67268467,  0.41059297,  0.80636424,  0.58491576,  0.69745988], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, x_train[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
