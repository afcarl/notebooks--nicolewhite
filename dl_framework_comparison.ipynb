{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Learning Framework Comparison\n",
    "\n",
    "**Logistic Regression** implemented in:\n",
    "\n",
    "* Theano\n",
    "* Tensorflow\n",
    "* PyTorch\n",
    "* Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 100\n",
    "num_feats = 40\n",
    "epochs = 10000\n",
    "learning_rate = .01\n",
    "\n",
    "x_train = np.random.normal(size=(num_samples, num_feats))\n",
    "y_train = np.random.randint(2, size=(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 40)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Static Computational Graphs\n",
    "\n",
    "Theano and Tensorflow require *static* computational graphs; they are defined *once* and executed over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Theano\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Shared variables](http://deeplearning.net/software/theano_versions/dev/tutorial/examples.html#using-shared-variables)\n",
    "* [Typed constructors](http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors)\n",
    "* [Gradients](http://deeplearning.net/software/theano/tutorial/gradients.html)\n",
    "* [Functions](http://deeplearning.net/software/theano/library/compile/function.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.dmatrix('x')\n",
    "y = T.dvector('y')\n",
    "\n",
    "w = theano.shared(np.zeros(num_feats))\n",
    "b = theano.shared(0.)\n",
    "\n",
    "yhat = 1 / (1 + T.exp(-T.dot(x, w) - b))\n",
    "loss = -y * T.log(yhat) - (1 - y) * T.log(1 - yhat)\n",
    "cost = loss.mean()\n",
    "\n",
    "dw, db = T.grad(cost, [w, b])\n",
    "\n",
    "train = theano.function(\n",
    "  inputs=[x, y],\n",
    "  outputs=cost,\n",
    "  updates=((w, w - learning_rate * dw), (b, b - learning_rate * db))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.6931471805599458\n",
      "Mean Loss 0.4907696315076982\n",
      "Mean Loss 0.4601996725846576\n",
      "Mean Loss 0.4449403242847565\n",
      "Mean Loss 0.4353546200341992\n",
      "Mean Loss 0.42868588008315733\n",
      "Mean Loss 0.4237745066451708\n",
      "Mean Loss 0.420024527532572\n",
      "Mean Loss 0.417088669765471\n",
      "Mean Loss 0.4147469388599722\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    mean_loss = train(x=x_train, y=y_train)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('Mean Loss', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorflow\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Variables](https://www.tensorflow.org/programmers_guide/variables)\n",
    "* [Placeholders](https://www.tensorflow.org/api_guides/python/reading_data#feeding)\n",
    "* [Gradients](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_computation)\n",
    "* [Session](https://www.tensorflow.org/api_docs/python/tf/Session#run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(num_samples, num_feats))\n",
    "y = tf.placeholder(tf.float32, shape=(num_samples))\n",
    "\n",
    "w = tf.Variable(tf.zeros((num_feats, 1)))\n",
    "b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "yhat = 1 / (1 + tf.exp(-tf.matmul(x, w) - b))\n",
    "loss = -y * tf.log(yhat) - (1 - y) * tf.log(1 - yhat)\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "dw, db = tf.gradients(cost, [w, b])\n",
    "\n",
    "new_w = w.assign(w - learning_rate * dw)\n",
    "new_b = b.assign(b - learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.693142\n",
      "Mean Loss 0.688257\n",
      "Mean Loss 0.688163\n",
      "Mean Loss 0.688143\n",
      "Mean Loss 0.688145\n",
      "Mean Loss 0.688138\n",
      "Mean Loss 0.688141\n",
      "Mean Loss 0.688138\n",
      "Mean Loss 0.688141\n",
      "Mean Loss 0.68814\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        mean_loss, _, _ = sess.run([cost, new_w, new_b], feed_dict={x: x_train, y: y_train})\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print('Mean Loss', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dynamic Computational Graphs\n",
    "\n",
    "PyTorch and Neon offer \"true\" automatic differientiation. The computational graph is *dynamic*; it is constructed on the fly during each epoch as we calculate the loss and weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PyTorch\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Variables and autograd](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd)\n",
    "* [Static vs dynamic graphs](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#tensorflow-static-graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "x = Variable(torch.from_numpy(x_train).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.from_numpy(y_train).type(dtype), requires_grad=False)\n",
    "\n",
    "w = Variable(torch.zeros(num_feats, 1), requires_grad=True)\n",
    "b = Variable(torch.zeros(1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.6931471824645996\n",
      "Mean Loss 0.6882584095001221\n",
      "Mean Loss 0.6881639361381531\n",
      "Mean Loss 0.6881466507911682\n",
      "Mean Loss 0.6881416440010071\n",
      "Mean Loss 0.6881399154663086\n",
      "Mean Loss 0.6881392598152161\n",
      "Mean Loss 0.688139021396637\n",
      "Mean Loss 0.6881389021873474\n",
      "Mean Loss 0.6881388425827026\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    yhat = 1 / (1 + torch.exp(-x.mm(w) - b))\n",
    "    loss = -y * torch.log(yhat) - (1 - y) * torch.log(1 - yhat)\n",
    "    cost = torch.mean(loss)\n",
    "    \n",
    "    cost.backward()  # <-- This is neat.\n",
    "\n",
    "    w.data -= learning_rate * w.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('Mean Loss', cost.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neon\n",
    "\n",
    "Relevant docs:\n",
    "\n",
    "* [Neon backend](https://neon.nervanasys.com/index.html/backends.html)\n",
    "* [Automatic differentiation](https://neon.nervanasys.com/index.html/backends.html#automatic-differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend, Autodiff\n",
    "\n",
    "be = gen_backend('cpu')\n",
    "\n",
    "w = be.zeros(num_feats)\n",
    "b = be.zeros(1)\n",
    "\n",
    "x = be.empty_like(x_train)\n",
    "y = be.empty_like(y_train)\n",
    "x[:] = x_train\n",
    "y[:] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss 0.692168\n",
      "Mean Loss 0.490721\n",
      "Mean Loss 0.46018\n",
      "Mean Loss 0.444929\n",
      "Mean Loss 0.435347\n",
      "Mean Loss 0.42868\n",
      "Mean Loss 0.42377\n",
      "Mean Loss 0.420021\n",
      "Mean Loss 0.417086\n",
      "Mean Loss 0.414745\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    yhat = 1 / (1 + be.exp(-be.dot(x, w) - b))\n",
    "    loss = -y * be.log(yhat) - (1 - y) * be.log(1 - yhat)\n",
    "    cost = be.mean(loss)\n",
    "\n",
    "    ad = Autodiff(op_tree=cost, be=be)\n",
    "    dw, db = ad.get_grad_tensor([w, b])\n",
    "    \n",
    "    new_w = w - learning_rate * dw\n",
    "    new_b = b - learning_rate * db\n",
    "    \n",
    "    w.copy(new_w)\n",
    "    b.copy(new_b)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('Mean Loss', cost.asnumpyarray()[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Abstractions\n",
    "\n",
    "### Keras\n",
    "\n",
    "External abstractions for Theano and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(num_feats,)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=num_samples, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PyTorch's `nn` module\n",
    "\n",
    "Internal abstractions for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, Sigmoid, CrossEntropyLoss\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "x = Variable(torch.from_numpy(x_train).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.from_numpy(y_train).type(dtype), requires_grad=False)\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(in_features=num_feats, out_features=1),\n",
    "    Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    yhat = model(x)\n",
    "    loss = -y * torch.log(yhat) - (1 - y) * torch.log(1 - yhat)\n",
    "    cost = torch.mean(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neon\n",
    "\n",
    "Internal abstractions for Neon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from neon.layers import Affine, GeneralizedCost\n",
    "from neon.initializers import Constant\n",
    "from neon.transforms import Logistic, CrossEntropyBinary\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum\n",
    "from neon.data import ArrayIterator\n",
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend('cpu', batch_size=num_samples)\n",
    "\n",
    "model = Model([\n",
    "    Affine(nout=2, init=Constant(0.), bias=Constant(0.), activation=Logistic())\n",
    "])\n",
    "\n",
    "cost = GeneralizedCost(CrossEntropyBinary())\n",
    "optimizer = GradientDescentMomentum(learning_rate=learning_rate, momentum_coef=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = ArrayIterator(x_train, y_train, nclass=2)\n",
    "model.fit(dataset, cost=cost, optimizer=optimizer, num_epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
